{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Dropout Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we'll practice adding regularization to the neural network with the use of dropout layers and batchnorm.  Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now before starting up, let's begin by changing our runtime type to GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let's set our device to cuda, and load up our FashionMNIST dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Make sure to transform the dataset `ToTensor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = None\n",
    "test = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Then, shape our data into batches of size 100, where each observation has consists of one channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_reshaped = train.data.reshape(-1, 100, 1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_reshaped = train.targets.reshape(-1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = list(zip(X_train_reshaped, y_reshaped))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refactoring Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now let's start off with the following neural network we had in the last lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, stride=1, kernel_size=5, padding=2)\n",
    "        self.conv2 = nn.Conv2d(6, 12, stride=1, kernel_size=3, padding=1)\n",
    "        self.batch_norm2d = nn.BatchNorm2d(12)  \n",
    "        self.l1 = nn.Linear(12*7*7, 64)\n",
    "        self.batch_norm1d = nn.BatchNorm1d(64)  \n",
    "        self.l2 = nn.Linear(64, 10)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        A1 = F.dropout2d(F.relu(self.conv1(X)), p = .1)\n",
    "        pooled_1 = F.avg_pool2d(A1, kernel_size = 2)\n",
    "        A2 = F.dropout2d(self.batch_norm2d(F.relu(self.conv2(pooled_1))),p = .1)\n",
    "\n",
    "        pooled_2 = F.avg_pool2d(A2, kernel_size = 2) # 16x2x2\n",
    "        reshaped = pooled_2.reshape(-1, 12*7*7)\n",
    "        L1 = F.dropout(self.batch_norm1d(F.relu(self.l1(reshaped))), p = .3)\n",
    "        L2 = self.l2(L1)\n",
    "        return F.log_softmax(L2, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We can initialize an instance of the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 1, 28, 28])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_batch = combined[0][0]\n",
    "first_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then pass through a batch of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_predictions = net(first_batch.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 10])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_predictions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's convert the neural network above to use nn.Sequential.  We'll get you started with the convolutional layers. Your task will be to flatten the data and add the linear layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "seq_net = nn.Sequential(\n",
    "    nn.Conv2d(1, 6, stride=1, kernel_size=5, padding=2),\n",
    "    nn.ReLU(),\n",
    "    nn.AvgPool2d(kernel_size = 2),\n",
    "    nn.Conv2d(6, 12, stride=1, kernel_size=3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.AvgPool2d(kernel_size = 2),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(12*7*7, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 10),\n",
    "    nn.LogSoftmax(dim = 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that `seq_net` returns 10 predictions for each observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_net(first_batch.float())[:1].shape\n",
    "\n",
    "# torch.Size([1, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Dropout "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so our neural network should look like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "seq_net = nn.Sequential(\n",
    "    nn.Conv2d(1, 6, stride=1, kernel_size=5, padding=2),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout2d(p = .1),\n",
    "    nn.AvgPool2d(kernel_size = 2),\n",
    "    nn.Conv2d(6, 12, stride=1, kernel_size=3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm2d(12),\n",
    "    nn.Dropout2d(),\n",
    "    nn.AvgPool2d(kernel_size = 2),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(12*7*7, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm1d(64), \n",
    "    nn.Dropout(),\n",
    "    nn.Linear(64, 10),\n",
    "    nn.LogSoftmax(dim = 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For the first convolutional sequence, let's have `conv > relu > dropout > avgpool`.  This way dropout is applied after our activation function.\n",
    "\n",
    "* For the second sequence, let's have `conv > relu > batchnorm > dropout > avgpool`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* By default, Pytorch will set a dropout rate with $p = .5$.  But for convolutional layers, the dropout rate is typically smaller.  So pass through the parameter $p = .2$ for each dropout function.\n",
    "\n",
    "* And, for the first linear layer apply the sequence of `Linear > Relu > Batch_norm1d > dropout`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now our neural network should be updated to look like the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (1): ReLU()\n",
       "  (2): Dropout2d(p=0.1, inplace=False)\n",
       "  (3): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (4): Conv2d(6, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (5): ReLU()\n",
       "  (6): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (7): Dropout2d(p=0.5, inplace=False)\n",
       "  (8): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (9): Flatten()\n",
       "  (10): Linear(in_features=588, out_features=64, bias=True)\n",
       "  (11): ReLU()\n",
       "  (12): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (13): Dropout(p=0.5, inplace=False)\n",
       "  (14): Linear(in_features=64, out_features=10, bias=True)\n",
       "  (15): LogSoftmax()\n",
       ")"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_net\n",
    "\n",
    "# Sequential(\n",
    "#   (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
    "#   (1): ReLU()\n",
    "#   (2): Dropout2d(p=0.1, inplace=False)\n",
    "#   (3): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
    "#   (4): Conv2d(6, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "#   (5): ReLU()\n",
    "#   (6): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "#   (7): Dropout2d(p=0.5, inplace=False)\n",
    "#   (8): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
    "#   (9): Flatten()\n",
    "#   (10): Linear(in_features=588, out_features=64, bias=True)\n",
    "#   (11): ReLU()\n",
    "#   (12): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "#   (13): Dropout(p=0.5, inplace=False)\n",
    "#   (14): Linear(in_features=64, out_features=10, bias=True)\n",
    "#   (15): LogSoftmax()\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's initialize our optimizer and train the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(new_net.parameters(), lr=.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(7):\n",
    "    for X_batch, y_batch in combined:\n",
    "        preds = new_net(X_batch.float())\n",
    "        loss = F.cross_entropy(preds, y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we are done training the neural network, it's time to see how it performs on the test data.  For this, we should turn off dropout as we wish to use all of our activations in making the predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we call `model.eval()` we are returned a copy of the neural network with the dropout layers not activated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_net = seq_net.eval()\n",
    "# eval_net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We can see that we are no longer in training mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_net.training\n",
    "\n",
    "# False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make predictions on the test set and check our model's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = eval_net(test_X_channel.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_predictions = torch.argmax(predictions, dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(test_y, max_predictions)\n",
    "# 0.8934"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "[Sequential Example Deep Lizard](https://deeplizard.com/learn/video/bH9Nkg7G8S0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
