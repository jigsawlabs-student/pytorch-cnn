{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refactoring with Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now that we've coded out a convolutional neural network in Pytorch, it's time to do some refactoring.  One of the issues with the current way that we are coding our neural network, is that the sequence that our data passes through is spread out between two different functions: `init` and `forward`.  In this lesson, we'll see how we can combine that sequence, and hopefully make our code more clear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rewriting our CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the neural network that we defined with by inheriting from `nn.Module` and declaring a class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, stride=1, kernel_size=5, padding = 2)\n",
    "        self.conv2 = nn.Conv2d(6, 12, stride=1, kernel_size=3, padding = 1)\n",
    "        self.L1 = nn.Linear(7*7*12, out_features=64)\n",
    "        self.L2 = nn.Linear(64, out_features=10)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        A1 = F.relu(self.conv1(X))\n",
    "        pooled_1 = F.avg_pool2d(A1, kernel_size = 2)\n",
    "        A2 = F.relu(self.conv2(pooled_1))\n",
    "        pooled_2 = F.avg_pool2d(A2, kernel_size = 2) # 16x2x2\n",
    "        reshaped_pool = pooled_2.reshape(-1, 7*7*12)\n",
    "        A3 = F.relu(self.L1(reshaped_pool))\n",
    "        A4 = self.L2(A3)\n",
    "        return F.log_softmax(A4, dim = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can write the entire neural network with the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(1, 6, stride=1, kernel_size=5, padding=2),\n",
    "    nn.ReLU(),\n",
    "    nn.AvgPool2d(kernel_size = 2),\n",
    "    nn.Conv2d(6, 12, stride=1, kernel_size=3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.AvgPool2d(kernel_size = 2),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(12*7*7, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 10),\n",
    "    nn.LogSoftmax(dim = 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See if you can follow how one translates to the other, and then in the next section we'll walk through some of the details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin with just comparing the convolutional layers of the neural network in class form, and with using Sequential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, stride=1, kernel_size=5, padding = 2)\n",
    "        self.conv2 = nn.Conv2d(6, 12, stride=1, kernel_size=3, padding = 1)\n",
    "    def forward(self, X):\n",
    "        A1 = F.relu(self.conv1(X))\n",
    "        pooled_1 = F.avg_pool2d(A1, kernel_size = 2)\n",
    "        A2 = F.relu(self.conv2(pooled_1))\n",
    "        pooled_2 = F.avg_pool2d(A2, kernel_size = 2) \n",
    "        return pooled_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "    nn.Conv2d(1, 6, stride=1, kernel_size=5, padding=2),\n",
    "    nn.ReLU(),\n",
    "    nn.AvgPool2d(kernel_size = 2),\n",
    "    nn.Conv2d(6, 12, stride=1, kernel_size=3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.AvgPool2d(kernel_size = 2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, with `Sequential`, each step in the neural network is defined inside of the `Sequential` parentheses in the function.  Just like we before, we initialize the convolutional layers with `nn.Conv2d`. \n",
    "\n",
    "One difference, is that now our activation and pooling functions, `ReLu` and `AvgPool2d` are initialized as an instance of a class.  We are no longer using the `F.relu` and `F.avg_pool2d`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the rest of the neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(1, 6, stride=1, kernel_size=5, padding=2),\n",
    "    nn.ReLU(),\n",
    "    nn.AvgPool2d(kernel_size = 2),\n",
    "    nn.Conv2d(6, 12, stride=1, kernel_size=3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.AvgPool2d(kernel_size = 2),\n",
    "    \n",
    "    nn.Flatten(),\n",
    "    \n",
    "    nn.Linear(12*7*7, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 10),\n",
    "    nn.LogSoftmax(dim = 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that in between the pooling and the linear layer is a call to `nn.Flatten`.  This reshapes our data from our 12 different 7x7 channels that are outputted from the pooling layer to have each observation be represented as a vector.  \n",
    "\n",
    "We previously achieved this in the `forward` method with the following."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "reshaped_pool = pooled_2.reshape(-1, 7*7*12)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whether to use Sequential or to declare the neural network in a class depends on personal preference.  However, it is good to have the ability to read both formats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson, we saw how to declare a neural network with the `nn.Sequential` method.  As we saw, one of the benefits about using Sequential is that now the entire flow is described in one sequence.\n",
    "\n",
    "In Sequential, every argument must come from the `nn` module.  Because of this, we switched from using our `F.relu` function and `F.avg_pool2d` functions.  We also learned about the `nn.Flatten` class, which flattens our data from matrix to vector form."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
