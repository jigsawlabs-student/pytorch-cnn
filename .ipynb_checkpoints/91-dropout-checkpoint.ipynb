{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With thousands of parameters trained in a neural network, it should not be a surprise that neural networks are prone to overfitting.  As we know, this means that our neural network can essentially memorize how to respond to features in the training set that do not generalize to future data.\n",
    "\n",
    "In this lesson we'll see two mechanisms to prevent overfitting in neural networks: dropout and batch normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Dropout "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout was [first described in 2014](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf).  Take a look at the neural network that we have to the left.  This is our standard neural network, where we feed data into our network, producing different activations from the neurons in the first layer, which are then passed as inputs into the second layer, and so on.  \n",
    "\n",
    "With dropout, displayed on the right, activations are randomly turned off throughout the network.  The idea is that by randomly turning off certain activations, our neural network does not become dependent on any specific neuron, which perhaps is specializing in certain features of the dataset.  So with dropout, we can better ensure that the predictions of our neural network are dependent upon multiple neurons, instead of a single one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"dropout-hinton.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We discard of activations with a probability of $p$, where generally $p = .5$.  One thing about dropout is that it is only applied at training time.  During testing, all of the activations in the neural network are used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When implementing dropout in our neural networks in Pytorch, there are a couple of things to keep in mind.  First, is that [it does not matter](https://groups.google.com/a/tensorflow.org/forum/#!topic/discuss/zt82ACSp2sw) if we apply dropout after our linear layer or after our activation layer, the result is the same.  Second, is that [we can apply](https://stats.stackexchange.com/questions/240305/where-should-i-place-dropout-layers-in-a-neural-network) dropout to a convolutional layer, but we should generally use a smaller percentage of dropout like $p = .1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, to avoid dropout during test time we can call `model.eval()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Batch Norm Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next thing that we can use to improve our model is batch normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./batch-norm-smoothe.png\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The takeaway from the image above is that, with batch normalization, we can increase the learning rate.  The reason is because when our loss drastically changes, we know that our step size will be large and our network is thus jumping off into some part of the weight space that may or not be performing well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how does batch norm achive a smoother change in our losses?  For this, we'll [look to a nice explanation](https://youtu.be/hkBa9pU-H48?t=2973) from Jeremy Howard.  We can think of our neural network essentially as a function of hundreds of thousands of different parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"neural-net-function.png\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With batch normalization, we add a vector to multiply the last layer of our neural network by, and a bias vector to for addition on this last term.  The result is that our neural network is now better able to shift around our neural network's outputs by changing the parameters in this batch normalization function, and can do so without coordinating changes to the much larger set of features in the rest of the neural network.  \n",
    "\n",
    "Once the neural network is steered in the correct direction through batch normalization, then the rest of the parameters are learned through gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./batch-norm-img.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In CNNs we can apply batch normalization on our last convolutional layer, and on the second to last linear layer.  Take a look at the example below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Linear > Relu > BatchNorm > Dropout`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Net(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Net, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(in_channels=1, out_channels=10,\n",
    "#                                kernel_size=5,\n",
    "#                                stride=1)\n",
    "#         self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "#         self.conv2_bn = nn.BatchNorm2d(20)\n",
    "#         self.dense1 = nn.Linear(in_features=320, out_features=50)\n",
    "#         self.dense1_bn = nn.BatchNorm1d(50)\n",
    "#         self.dense2 = nn.Linear(50, 10)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = F.max_pool2d(F.relu(self.conv1(x), 2))\n",
    "#         x = F.max_pool2d(F.relu((self.conv2_bn(self.conv2(x)), 2)\n",
    "#         x = x.view(-1, 320) #reshape\n",
    "#         x = F.relu(self.dense1_bn(self.dense1(x)))\n",
    "#         x = F.relu(self.dense2(x))\n",
    "#         return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources \n",
    "\n",
    "[StackOverflow Dropout](https://stats.stackexchange.com/questions/240305/where-should-i-place-dropout-layers-in-a-neural-network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Raschka Dropout](https://groups.google.com/a/tensorflow.org/forum/#!topic/discuss/zt82ACSp2sw)\n",
    "\n",
    "[Batchnorm example](https://discuss.pytorch.org/t/batch-normalization-of-linear-layers/20989/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
