{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Dropout Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we'll practice adding regularization to the neural network with the use of dropout layers and batchnorm.  Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by loading the datset, and setting our device to cuda. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = datasets.FashionMNIST(\"\", train = True, download = True,\n",
    "                      transform = transforms.Compose([transforms.ToTensor()]))\n",
    "test = datasets.FashionMNIST(\"\", train = False, download = True,\n",
    "                       transform = transforms.Compose([transforms.ToTensor()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Then we shape our data into batches of size 100, where each observation has consists of one channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_reshaped = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And reshape the y data into batches of 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_reshaped = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally zip the data together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = list(zip(X_train_reshaped, y_reshaped))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refactoring Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now we'll start off with the following neural network we had in the last lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, stride=1, kernel_size=5, padding=2)\n",
    "        self.conv2 = nn.Conv2d(6, 12, stride=1, kernel_size=3, padding=1)\n",
    "        self.batch_norm2d = nn.BatchNorm2d(12)  \n",
    "        self.l1 = nn.Linear(12*7*7, 64)\n",
    "        self.batch_norm1d = nn.BatchNorm1d(64)  \n",
    "        self.l2 = nn.Linear(64, 10)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        A1 = F.dropout2d(F.relu(self.conv1(X)), p = .1)\n",
    "        pooled_1 = F.avg_pool2d(A1, kernel_size = 2)\n",
    "        A2 = F.dropout2d(self.batch_norm2d(F.relu(self.conv2(pooled_1))),p = .1)\n",
    "\n",
    "        pooled_2 = F.avg_pool2d(A2, kernel_size = 2) # 16x2x2\n",
    "        reshaped = pooled_2.reshape(-1, 12*7*7)\n",
    "        L1 = F.dropout(self.batch_norm1d(F.relu(self.l1(reshaped))), p = .3)\n",
    "        L2 = self.l2(L1)\n",
    "        return F.log_softmax(L2, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We can initialize an instance of the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 1, 28, 28])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_batch = combined[0][0]\n",
    "first_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then pass through a batch of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_predictions = net(first_batch.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 10])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_predictions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's convert the neural network above to use `nn.Sequential`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "seq_net = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that `seq_net` returns 10 predictions for each observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_net(first_batch.float())[:1].shape\n",
    "\n",
    "# torch.Size([1, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Dropout "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now copy the sequential neural network from the above into the empty cell below, and update the neural network according to the following specifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For the first convolutional sequence, let's have `conv > relu > dropout > avgpool`.  This way dropout is applied after our activation function.\n",
    "\n",
    "* For the second sequence, let's have `conv > relu > batchnorm > dropout > avgpool`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* By default, Pytorch will set a dropout rate with $p = .5$.  But for convolutional layers, the dropout rate is typically smaller.  So pass through the parameter $p = .2$ for each dropout function.\n",
    "\n",
    "* And, for the first linear layer apply the sequence of `Linear > Relu > Batch_norm1d > dropout`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_seq_net = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now our neural network should be updated to look like the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (1): ReLU()\n",
       "  (2): Dropout2d(p=0.1, inplace=False)\n",
       "  (3): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (4): Conv2d(6, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (5): ReLU()\n",
       "  (6): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (7): Dropout2d(p=0.5, inplace=False)\n",
       "  (8): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (9): Flatten()\n",
       "  (10): Linear(in_features=588, out_features=64, bias=True)\n",
       "  (11): ReLU()\n",
       "  (12): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (13): Dropout(p=0.5, inplace=False)\n",
       "  (14): Linear(in_features=64, out_features=10, bias=True)\n",
       "  (15): LogSoftmax()\n",
       ")"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_seq_net\n",
    "\n",
    "# Sequential(\n",
    "#   (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
    "#   (1): ReLU()\n",
    "#   (2): Dropout2d(p=0.1, inplace=False)\n",
    "#   (3): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
    "#   (4): Conv2d(6, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "#   (5): ReLU()\n",
    "#   (6): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "#   (7): Dropout2d(p=0.5, inplace=False)\n",
    "#   (8): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
    "#   (9): Flatten()\n",
    "#   (10): Linear(in_features=588, out_features=64, bias=True)\n",
    "#   (11): ReLU()\n",
    "#   (12): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "#   (13): Dropout(p=0.5, inplace=False)\n",
    "#   (14): Linear(in_features=64, out_features=10, bias=True)\n",
    "#   (15): LogSoftmax()\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's initialize our optimizer with a learning rate of `.0005`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(updated_seq_net.parameters(), lr=.0005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's intialize the loss for the neural network as cross entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "cross_entropy_loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then write the training loop for the network.  Set the number of epochs equal to 7, and print the loss after each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write training loop here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we are done training the neural network, it's time to see how it performs on the test data.  For this, we should turn off dropout as we wish to use all of our activations in making the predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we call `model.eval()` we are returned a copy of the neural network with the dropout layers not activated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_net.eval()\n",
    "# eval_net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We can see that we are no longer in training mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_net.training\n",
    "\n",
    "# False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make predictions on the test set and check our model's accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> First convert the feature test data, by adding a channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X_channel = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then make the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then find the `arg max predictions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_predictions = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, use sklearn to calculate the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 0.8934"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "[Sequential Example Deep Lizard](https://deeplizard.com/learn/video/bH9Nkg7G8S0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
